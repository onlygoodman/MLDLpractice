{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_cfr006.ipynb",
      "provenance": [],
      "mount_file_id": "1nm4SsXDnkSDl3uADL9COLamLsPAmQrF_",
      "authorship_tag": "ABX9TyPFIQ9r4OsvzSzOW6R/pbKI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlygoodman/MLDLpractice/blob/master/project_cfr006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Isolation Forest\" 모델 사용\n",
        "001 feature 골라서 사용하면서 + pca 사용하여 최고점수 도출시도\n",
        "\n",
        "002 코사인, 피어슨 유사도 특성추가\n",
        "\n",
        "003 -\n",
        "\n",
        "004 Isolation Forest가 Robust모델에 비해 f1, auc 스코어 간의 격차가 적어 hyperparameter값 세부조정 시도\n",
        "\n",
        "- Isolation Forest h-params는 nes=150근처 pca는 사용하지 않을 때 성능이 가장 좋았다. -> 최대한 독립적인 특성만 사용하면서 학습시켰기 때문이라고 생각\n",
        "\n",
        "005 성능 개선을 위해서는 특성추가가 필요하다고 생각 11, 12, 17에서\n",
        "2, 4, 9, 10, 11, 12, 16, 17, 18 으로 특성 늘려서 학습\n",
        "\n",
        "006 4,9번특성이 성능이 잘나와서 4,9,11,12,17,31,32 특성을 사용하고 LOF와 OCS 모델 추가 사용하여 학습\n",
        "\n"
      ],
      "metadata": {
        "id": "GBTAnPQrEW7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DlHtVtnQECWj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import permutations,combinations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "\n",
        "import math #차후 factorial 계산 등 수학적인 계산을 위한 import\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import PolynomialFeatures #특성공학을 사용하기 위한 import\n",
        "from sklearn.preprocessing import StandardScaler #전처리된 데이터들의 scale을 맞춰주기 위한 import\n",
        "from sklearn.decomposition import PCA #차원 축소 - 주성분의 개수에 따른 최적화 모델 구현을 위한 import\n",
        "\n",
        "\n",
        "\n",
        "#model\n",
        "from sklearn import svm\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import KMeans\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score #이후 test.csv 파일의 predict 자료의 score을 매기기 위한 import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train dataset은 Label이 존재하지 않음\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/ai7/train.csv\")\n",
        "val_df  = pd.read_csv(\"/content/drive/MyDrive/ai7/val.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ai7/test.csv\")\n",
        "submit = pd.read_csv(\"/content/drive/MyDrive/ai7/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "JRnMJVzsEIU6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clist = list(train_df.columns)\n",
        "print(clist)\n",
        "\n",
        "train_data = train_df[clist[1:]].to_numpy()\n",
        "val_data = val_df[clist[1:]].to_numpy()\n",
        "val_target = val_df['Class'].to_numpy()\n",
        "\n",
        "print(val_target)\n",
        "\n",
        "valcount1=0\n",
        "valcountm1=0\n",
        "for i in val_target:\n",
        "  if i==1:\n",
        "    valcount1+=1\n",
        "  else:\n",
        "    valcountm1+=1\n",
        "\n",
        "print(valcount1, valcountm1)\n",
        "\n",
        "outliers_fraction = valcount1/(valcount1+valcountm1)\n",
        "print(\"outliers_fraction =\",outliers_fraction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZJiZLavELjX",
        "outputId": "010d5f7e-c556-4df9-d6b7-b76a0a67d6fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ID', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30']\n",
            "[0 0 0 ... 0 0 0]\n",
            "30 28432\n",
            "outliers_fraction = 0.0010540369615627855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코사인유사도 구하기"
      ],
      "metadata": {
        "id": "etI6DUKsKUG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_abnor = val_df[val_df['Class']==1][clist[1:]]\n",
        "abnor_vector = val_abnor.mean().to_numpy()\n",
        "\n",
        "def cos_sim(a, b):\n",
        "  return dot(a, b)/(norm(a)*norm(b))\n",
        "\n",
        "def cosfeature(data):\n",
        "  datacs=[]\n",
        "  for i in data[clist[1:]].to_numpy():\n",
        "    datacs.append(cos_sim(abnor_vector, i))\n",
        "  data['V31']=datacs\n",
        "\n",
        "cosfeature(train_df)\n",
        "cosfeature(val_df)\n",
        "cosfeature(test_df)\n"
      ],
      "metadata": {
        "id": "PtAsOqdALGZ5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "피어슨유사도 구하기"
      ],
      "metadata": {
        "id": "bYm4ee1x3YTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pearson_similarity(a, b):\n",
        "    return np.dot((a - np.mean(a)), (b - np.mean(b))) / ((np.linalg.norm(a - np.mean(a))) * (np.linalg.norm(b - np.mean(b))))\n",
        "\n",
        "def pearsonfeature(data):\n",
        "  datacs=[]\n",
        "  for i in data[clist[1:]].to_numpy():\n",
        "    datacs.append(pearson_similarity(abnor_vector, i))\n",
        "  data['V32']=datacs\n",
        "\n",
        "pearsonfeature(train_df)\n",
        "pearsonfeature(val_df)\n",
        "pearsonfeature(test_df)\n",
        "\n",
        "clist = list(train_df.columns)"
      ],
      "metadata": {
        "id": "1ibefjok3aJP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(clist))\n",
        "print(clist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FrxVSnmPsS6",
        "outputId": "6bc751c0-601c-4e54-ff57-473386516d06"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n",
            "['ID', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pSwRkqMsEVcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predata(data):\n",
        "  return data.drop(columns=['ID'])[col]\n",
        "\n",
        "def get_pred_label(model_pred):\n",
        "    # IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n",
        "    model_pred = np.where(model_pred == 1, 0, model_pred)\n",
        "    model_pred = np.where(model_pred == -1, 1, model_pred)\n",
        "    return model_pred\n",
        "\n",
        "scoreboard={}\n",
        "maxscoreboard=[]\n",
        "th_score=0 #제출 파일 저장 문턱점수 \n",
        "maxscore=0 #지난회차 최고점수\n",
        "testname=\"004\"\n",
        "poly_dim =1\n",
        "testnum = 0\n",
        "\n",
        "for k in range(1, 2):\n",
        "  for ctu in list(combinations([4,9], k)):\n",
        "    \n",
        "    col = [clist[i] for i in [11,12,17,31,32]+list(ctu)]\n",
        "    \n",
        "    train_x = predata(train_df)\n",
        "    val_x = predata(val_df) # Input Data\n",
        "    val_y = val_df['Class'] # Label\n",
        "\n",
        "    test_x = predata(test_df)\n",
        "\n",
        "    for i in range(len(col)-1, len(col)+1):\n",
        "      ss = StandardScaler()\n",
        "      ss.fit(train_x)\n",
        "      train_x = ss.transform(train_x)\n",
        "      val_x = ss.transform(val_x)\n",
        "      test_x = ss.transform(test_x)\n",
        "\n",
        "      pca = PCA(n_components = i)\n",
        "      pca.fit(train_x)\n",
        "      train_pca = pca.transform(train_x)\n",
        "      val_pca = pca.transform(val_x)\n",
        "      test_pca = pca.transform(test_x)\n",
        "\n",
        "      # anomaly_algorithms = [(\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
        "      #                     (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",gamma=0.1)),\n",
        "      #                     (\"Isolation Forest\", IsolationForest(max_samples=len(train_pca), contamination=outliers_fraction,random_state=42)),\n",
        "      #                     (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))]\n",
        "\n",
        "      contamination_controlnumber=0\n",
        "      \n",
        "      nes=150\n",
        "\n",
        "      ##lof hp\n",
        "      neig=35\n",
        "\n",
        "      ##ocs hp\n",
        "      gam=0.1\n",
        "      ker=\"rbf\"\n",
        "\n",
        "      model_if = IsolationForest(n_estimators=nes, max_samples=len(train_pca), contamination=outliers_fraction, random_state=42, verbose=0)\n",
        "      model_ocs = svm.OneClassSVM(nu=outliers_fraction, kernel=ker ,gamma=gam)\n",
        "      model_rc = EllipticEnvelope(contamination=outliers_fraction + contamination_controlnumber)\n",
        "      model_lof = LocalOutlierFactor(n_neighbors=neig, contamination=outliers_fraction)\n",
        "\n",
        "      # (\"One-Class SVM\",model_ocs) \n",
        "      # model_list=[(\"Isolation Forest\", model_if), (\"Robust covariance\", model_rc),(\"Local Outlier Factor\", model_lof),(\"One-Class SVM\", model_ocs)]\n",
        "      model_list=[(\"IF\", model_if), (\"RC\", model_rc),(\"LOF\", model_lof),(\"OCS\", model_ocs)]\n",
        "      \n",
        "      for name, m in model_list:\n",
        "        \n",
        "        testnum+=1\n",
        "        m.fit(train_pca)\n",
        "\n",
        "        if name == \"LOF\":\n",
        "            val_pred = m.fit_predict(val_pca)\n",
        "        else:\n",
        "            val_pred = m.predict(val_pca)\n",
        "\n",
        "        val_pred = get_pred_label(val_pred)\n",
        "        val_score = f1_score(val_y, val_pred, average='macro')\n",
        "        val_auc_score = roc_auc_score(val_y, val_pred)\n",
        "        scoreboard[\"model:{}, pca-ncom:{}, contamination:{}\".format(name, i, outliers_fraction + contamination_controlnumber)]=\\\n",
        "        \"f1-score : {}, auc-score : {}\".format(round(val_score, 4)*100, round(val_auc_score, 4)*100)\n",
        "\n",
        "        if val_score >= maxscore-0.0000001:\n",
        "          maxscore = val_score\n",
        "          th_score = val_score\n",
        "          maxscoreboard.append(\"no.{}-f1-score:{},\\t\\t auc-score:{}\".format(testnum, val_score, val_auc_score))\n",
        "          print(\"----------------------------------------------------\")\n",
        "          print(\"!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "          print(\"----------------------------------------------------\")\n",
        "\n",
        "        print(\"no.{} model_{}\\t---------------- \\tfeature:{},k:{}\\t----------------tmax-{}\".format(testnum, name, col, k,maxscore))\n",
        "\n",
        "        if name == model_list[0][0]:    #is\n",
        "          print('pca-ncom:{}, n-esti:{}'.format(i, nes),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        elif name == model_list[1][0]:  #rb\n",
        "          print('pca-ncom:{}'.format(i),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        elif name == model_list[2][0]:  #lof\n",
        "          print('pca-ncom:{}, neig:{}'.format(i, neig),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        else:                           #ocs\n",
        "          print('pca-ncom:{}, ganma:{}, kenel:{}'.format(i, gam, ker),'\\n',\n",
        "              'f1-score : {},\\t\\tauc-score : {}'.format(round(val_score, 4)*100, round(val_auc_score, 4)*100))\n",
        "          \n",
        "        # print(f'Validation F1 Score : [{val_score}]')\n",
        "        # print(classification_report(val_y, val_pred))\n",
        "        print()\n",
        "\n",
        "      if val_score>th_score:\n",
        "        test_pred = m.predict(test_pca) # model prediction  \n",
        "        test_pred = get_pred_label(test_pred)\n",
        "        submit['Class'] = test_pred\n",
        "        submit.to_csv('./submit{}_no.{}_model_{}.csv'.format(testname, testnum, name), index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAk3cqMAc0Vb",
        "outputId": "7f22571d-5b77-4f8b-f30f-e52a088a5302"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.1 model_Isolation Forest \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8385338130459596\n",
            "pca-ncom:5, n-esti:150 \n",
            " f1-score : 83.85000000000001,\t\tauc-score : 84.98\n",
            "\n",
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.2 model_Robust covariance \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8817038840461091\n",
            "pca-ncom:5 \n",
            " f1-score : 88.17,\t\tauc-score : 84.99\n",
            "\n",
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.3 model_Local Outlier Factor \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:5, neig:35 \n",
            " f1-score : 89.99000000000001,\t\tauc-score : 89.99000000000001\n",
            "\n",
            "no.4 model_One-Class SVM \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:5, ganma:0.1, kenel:rbf \n",
            " f1-score : 71.48,\t\tauc-score : 78.28\n",
            "\n",
            "no.5 model_Isolation Forest \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:6, n-esti:150 \n",
            " f1-score : 84.98,\t\tauc-score : 84.98\n",
            "\n",
            "no.6 model_Robust covariance \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:6 \n",
            " f1-score : 89.64,\t\tauc-score : 88.32\n",
            "\n",
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.7 model_Local Outlier Factor \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:6, neig:35 \n",
            " f1-score : 89.99000000000001,\t\tauc-score : 89.99000000000001\n",
            "\n",
            "no.8 model_One-Class SVM \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V4'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:6, ganma:0.1, kenel:rbf \n",
            " f1-score : 71.48,\t\tauc-score : 78.28\n",
            "\n",
            "no.9 model_Isolation Forest \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:5, n-esti:150 \n",
            " f1-score : 79.80000000000001,\t\tauc-score : 78.32000000000001\n",
            "\n",
            "no.10 model_Robust covariance \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.8998944850872257\n",
            "pca-ncom:5 \n",
            " f1-score : 88.88000000000001,\t\tauc-score : 84.99\n",
            "\n",
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.11 model_Local Outlier Factor \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:5, neig:35 \n",
            " f1-score : 91.66,\t\tauc-score : 91.66\n",
            "\n",
            "no.12 model_One-Class SVM \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:5, ganma:0.1, kenel:rbf \n",
            " f1-score : 71.48,\t\tauc-score : 78.28\n",
            "\n",
            "no.13 model_Isolation Forest \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:6, n-esti:150 \n",
            " f1-score : 89.99000000000001,\t\tauc-score : 89.99000000000001\n",
            "\n",
            "no.14 model_Robust covariance \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:6 \n",
            " f1-score : 88.17,\t\tauc-score : 84.99\n",
            "\n",
            "----------------------------------------------------\n",
            "!!!!!!!!!!!!!!!!!!!!!max score!!!!!!!!!!!!!!!!!!!!!!\n",
            "----------------------------------------------------\n",
            "no.15 model_Local Outlier Factor \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:6, neig:35 \n",
            " f1-score : 91.66,\t\tauc-score : 91.66\n",
            "\n",
            "no.16 model_One-Class SVM \t\t ------------\tfeature:['V11', 'V12', 'V17', 'V31', 'V32', 'V9'],k:1\t\t\tmax-0.9165787375726882\n",
            "pca-ncom:6, ganma:0.1, kenel:rbf \n",
            " f1-score : 70.95,\t\tauc-score : 78.27\n",
            "\n"
          ]
        }
      ]
    }
  ]
}